\documentclass{sig-alternate}
\usepackage{url}
\usepackage{tabulary}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{OzCHI}{'15, December 07-10, 2015, Melbourne, Australia}
%\CopyrightYear{2015} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Comparing usability of concurrency models}

\numberofauthors{1}
\author{
\alignauthor
Peter Davis, Ben Swift, Henry Gardner\\
\vskip .3em
\affaddr{Research School of Computer Science}\\
\affaddr{Australian National University}\\
\affaddr{Canberra, Australia}\\
\vskip .3em
\email{\{peter.davis,ben.swift,henry.gardner\}@anu.edu.au}
}

\maketitle

\begin{abstract}
Abstract goes here.
\end{abstract}

\section{Introduction}

% TODO: or parallel, or distributed

It is a truth, universally acknowledged, that every algorithm must be
in want of an efficient concurrent implementation. The rise of
multicore computing has highlighted the difficulty of constructing
such an implementation, so that in many application domains our
capacity (the number of cores at our disposal) continues to outpace
our capability (the ability to use those cores efficiently).
Fundamentally, this is a human factors problem: as Herb Sutter and
James Larus point out, ``concurrency requires programmers to think in
a way humans find difficult''\cite[p56]{sutter_software_2005}.

In response to this challenge many programming models, tools and even
new languages have been created to aid software developers in in
developing concurrent programs. In many cases, these tools take the
form of Application Program Interfaces (APIs), where a software
``interface'' is presented to developers to abstract away (some of)
the complexity of concurrent execution and synchronisation. Although
APIs are not the same as graphical interfaces, there is a growing
interest in in the HCI community (e.g. a special interest group on API
Usability at \emph{CHI~'09}\cite{daughtry_api_2009}) that these
interfaces are worth considering from a HCI perspective.

This paper explores the usability of ``interfaces'' for concurrent
programming through the lens of Green's \emph{Cognitive Dimensions of
  Notations} framework\cite{green89:_cognit_dimen_notat}. In
particular, we consider four popular approaches:
threads, Bulk Synchronous Parallel (BSP), Communicating Sequential
Processes (CSP) and Message Passing Interface (MPI).  We begin by
situating these approaches in the context of the wider concurrent
programming landscape, before examining the usability (in terms of
cognitive dimensions) of each one in detail. We conclude by\ldots

Our claim in this paper is that the difficulty of developing efficient
concurrent software is a \emph{real} problem, and that it is an
\emph{HCI} problem. By considering the usability of these APIs from an
HCI perspective we hope to show how considering the human factors of
these programmer interfaces can reveal areas for improvement in the
way we support developers in taking advantage of the computational
resources at their disposal.

% The field of concurrent processing is notable for having multiple
% applications, architectures and problems that sequential programming
% paradigms do not have. This exploration is interesting as the primary
% constraint in the development of quality (as defined by \cite{9126})
% concurrent systems is the difficulty that programmers encounter when
% developing concurrent systems.

% We propose a conceptual model similar to the Open Systems Interconnect
% (OSI) Model but intended for use in understanding and categorising
% concurency concepts. This model is called the Concurrency Layers Model
% (CLM) and we make the argument that all implemented concurrent
% solutions utilise each layer of the concurrency model. We then
% categorise some leading concurrent tools using the concurrent layers,
% showing where in CLM each tool is placed.





\section{Common Concurrency Issues}
\subsection{General Issues}
\subsubsection{Concurrency Definitions}

There exists some confusion around the categorisation and definiton of concurrent systems. Some users mistake concurrency for paralellism \cite{pike13:_concur_paral}, where the key distinction is that concurrent programs have multiple processes, however concurrency makes no promises about when or how these processes are run. In order for a program to be parallel it must physically be running more than a single process at a time, not just be architected as a set of seperate processes that run sequentially.

The rise of javascript and its event-driven concurrency model has meant that the ``non-blocking'' function call has become a widely used concurrency tool. There is some confusion as to the difference between asynchronous and parallel concurrent behaviour

\subsubsection{Deadlock}
\cite{shanlu08:_learn_mistak_compr_study_real} examines the problem patterns, manifestation and solutions of 105 randomly selected real world concurrency bugs. This paper only looked at threading and pointers (as this is the dominant concurrent programming paradigm), but the issues explored are broadly applicable to all concurrent programming models.
Defined by \cite{shanlu08:_learn_mistak_compr_study_real} as:``Deadlock occurs when two or more operations circularly wait for each other to release the acquired resource''. Whilst being the most well known 

\subsubsection{Atomicity Violation}
Defined by \cite{shanlu08:_learn_mistak_compr_study_real} as: ``The desired serializability among multiple memory accesses is violated (i.e. a code region is intended to be atomic, but the atomicity is not enforced during execution)''. \cite{shanlu08:_learn_mistak_compr_study_real} found that out of 105 studied concurrency bugs, 

\subsubsection{Order Violation}
Defined by \cite{shanlu08:_learn_mistak_compr_study_real} as:``The desired order between two (groups of) memory accesses is flipped (i.e., A should always be executed before B, but the order is not enforced during execution)''. This is also known as a race condition. 

\subsubsection{Livelock}
Defined by \cite{oracle:_starv_livel} as when: ``Threads are not blocked - they are simply too busy responding to one another to continue work''. This issue was not identified specifically by \cite{shanlu08:_learn_mistak_compr_study_real} and so its real world prevalence is unclear.

\subsection{Distributed Issues}
The issues that distributed concurrent systems face include all of the singular system problems, but are affected by the vagaries of both a more complicated underying physical interconnection sysstem and a greater number of individual components potentially affected by failure. The eight fallacies of distributed computing are a widely used overview of the problems affecting distributed systems:

\begin{enumerate}
\item The network is reliable
\item Latency is zero
\item Bandwidth is infinite
\item The network is secure
\item Topology doesn't change
\item There is one administrator
\item The transport cost is zero
\item The network is homogenous
\end{enumerate}

\subsubsection{Consistency and Replication}
This refers to the problem of keeping global state synchronised across multiple processors, and doing so in a fashion that does not adversely affect performance.

\subsubsection{Reliability}
In non-distributed systems it is incredibly remote that calls to local memory or local cache will fail, however this is not true when running over a network. 

\section{Concurrency Layers}
To aid in discussion of concurrent systems, we propose the usage of the Concurrent Layers Model (CLM) to help describe how the various concurrent tools aid in creation of concurrent systems. Similar to the OSI model of splitting the network stack by each layer, each layer relies on the previous layer in order to function properly and provide services to the next layer up.

\begin{tabulary}{\linewidth}{LLLLL}
  & Layer & Example \\ \hline
  1 & Physical &  Physical computing hardware, network connnections \\
  2 & Communication & MPI, ZeroMQ, Pointers \\
  3 & Concurrency Model  & BSP, Actor, CSP, Threads \\
  4 & Process Graph & Algorithmic skeletons, Directed Acyclic Graphs (DAGs) \\
  5 & Solution Algorithm & FFT, MapReduce, Particle-In-Cell (PIC)
\end{tabulary}

\section{Concurrency Libraries and Tools}
\subsection{MPI}
MPI was orignally conceived as an effort to standardise the increasingly disparate communication protocols that were being developed by each Massively Parallel Processor (MPP) manufacturer \cite{g.96:_pvm_mpi_compar_featur}. It quickly became much more than a communication library, providing a set of implicit concurrency models, algorithmic skeletons and performance enhancing tools. In the parlance of the concurrency layers provided above, this concurrency tool spans multiple concurrency layers (2-4) and this flexibility is likely why MPI has become the de-facto tool for supercomputing applications.

\subsection{Threads}
Threads represent independent execution contexts within a computing system. They exist at the third level of the concurrency layer, and are an operating system concurrency primitive. Threads were the original construct used to execute concurrent programming and are arguably still the most prevalent, being the primarily method by which most operating systems coordinate the various operations that must be completed to preseent a responsive interface to the user.

Threads use pointers to shared data structures in memory, controlling access to these shared data structures via a set of ownership concepts (e.g. locks, semaphore, mutexes). This shared memory communication method has been espoused by some as being a positive \cite{john90:_munin}: ``Shared memory programs are easier to develop than distributed memory (message passing) programs, because the programmer need not worry about the explicit movement of data.'' 

\subsection{Go Channels}
Go is a relatively new programming language having been released for public usage in 2009 \cite{go:_frequen_asked_quest}, it is a statically typed, compiled language with first class support for concurrency. The ``go'' statement which generates lightweight threads called ``goroutines'', and its associated ``channel'' statement which provides communications between goroutines.

Go's channels are inspired by the Communicating Sequential Processes (CSP) which is a concurrency paradigm created by \cite{Hoare:1978:CSP:359576.359585} in the late 1970's. CSP was designed to formalize the patterns of interaction in concurrent systems, providing a syntax formal method of describing multi-process communication with the aim of provable multi-process correctness.

\subsection{Particle In Cell Simulations}
Particle-In-Cell (PIC) simulations are used by physicists when modelling and understanding particle behaviour in the presence of various fields (e.g. electromagnetic, gravity). PIC algorithms spread the high computational load across multiple processors in order to efficiently calculate the systems behaviour through time, and exist at the application layer in the CLM. These algorithms are concurrently designed, but require a parallel system in order to complete their calculations in a reasonable time frame. 

\subsection{Apache Spark}
Developed from work on Resilient Distributed Datasets (RDDs)  \cite{zaharia14:_archit_fast_gener_data_proces_large_clust},  Apache Spark is intended as a general purpose cluster computing environment. It exists at the application layer by abstracting over the lower concurrency layers, concentrates on problems that require a particular graph setup (master-slave) and provides four concurrent tools for users of the system:

\begin{itemize}
\item RDD - A distributed and fault tolerant data storage abstraction
\item Shared Variables - Allows parallel operations to access shared state
\item Application Framework - Allows users to create parallel applications without being concerned with the lower concurrency layers
\item Interoperability - Allows data storage and retrieval from popular data storage solutions such as Hadoop or HBase
\end{itemize}

\section{HCI in Concurrency}
HCI is concerned with the usage, implementation and perception of computers by humans. This paper is interested in the usability of concurrent systems. 

Concurrent programming poses a particuarly complicated set of problems for programmers due to its non-sequential nature, being hard to reason about, hard to implement and hard to maintain. 


Usability is typically considered as a metric of software quality, and is defined by \cite{9126} as: ``The capability of the software product to be understood, learned, used and attractive to the user, when used under specified conditions.'' Normally this definition is aimed at end users encountering finished software products, rather than being aimed at the programmer who is engaged in making the software.

Programmers face a separate set of usability issues when writing software.  software patterns are notorious for being hard to reason about and hence have poor usability.

In order to explore the usability of various concurrency models we utilise the Cognitive Dimensions \cite{green89:_cognit_dimen_notat} (CD) framework, 

which was initially developed to explore the productivity of visual programming environments but has been used more broadly in areas such as programming language evaluation \cite{enpl}, and 

are the users’ intended activities adequately supported by the structure of the information artifact

We consider some of the tools used for concurrent programming and examine how these tools aim to provide a more productive, safe or understandable environment for a programmer wishing to create concurrent programs. We approach these aims from the standpoint that many of the porblems present in concurrent programming arise as a result of a lack of 'usability' of concurrent paradigms and tools.



\subsection{Hidden Dependencies}

Recovering the mental representation from the concurrent program is difficult, as there is not necessarily any sequential manner in which the program may operate. As such, for each process in a concurrent system there are hidden dependencies in the form of the other processes in the system, which may be in any number of states and producing any number of communications to the process being considered.

\subsection{Viscosity/Fluidity}
The viscoisity of a concurrent program is hard to generalize. On one hand, the program may be very fluid if it is an operation that is replicated across many nodes with little or no variance in each nodes funciton. In this case then the program is very fluid as any changes made only need to be made to a single part of the code, and this code will be replicated to all processes. However, if the concurrent program has many different nodes with different behaviours and functions then it may be a viscous procedure to any part of the program, as the flow on effects from that change will need to be tracked and accounted for in the downstream or dependent nodes.

\subsection{Premature commitment}
Concurrent programs encourage some degree of premature commitment from the programmer, particularly along the concurrecy layers. Before being able to attack the root of the probelm they are trying to solve, the programmer will need to take into account the concurrency model, the skeleton algorithm, the physical hardware available, and the communication method. Whether the problem is being solved via a directed acyclic graph (DAG) topology or via a grid topoology will determine the structure of the concurrency model, considerations such as whether messages between processes can be buffered or must be synchronous will affect how they can write the solution.

\subsection{Role Expressiveness}
Role expressiveness is related to how easily the user can discern the purpose of a particular component, and hence create a mental model of how the system works. Concurrent systems do not offer any particular advantage or disadvantage in this regard; they may provide simpler methods of expressiveness by having each process perform a single easily identifiable role, allowing the user to identify the system purpose as a simple workflow, but more complicdated concurrent systems where each process handles multiple functions within the system may cause the programmer difficulty in identify the role of particular system components. 

Increasing the role expressivenesss of a concurrent system can be thought of as decreasing the number of functions that a particular process is required to complete, thereby allowing the programmer to easily identify the role of each process and its place within the overall system. This may come a cost of less performant system, greater hardware requirements to support the greater number of simpler concurrency nodes, or may be completely impossible if the solution demands. 

\subsection{Hard Mental Operations}
Not all mental operations are equivalent, and non-sequential program execution is commonly cited as a common hard mental opeation. Many of the concurrent tools try to abstract away as much of the non-sequential operations as possible, leaving the programmer to only need to consider a set of related sequential steps. 

\section{Applying CD to Select Concurrent Models}
The CD framework allows us to examine some (hopefully) orthogonal congnitive concepts and how they relate to some concurrent models. Three models from different levels of the concurrency layer are chosen and examined.

\subsection{MPI}
MPI has a minimum amount of abstractions, using a large but fairly basic api for programmers to utilise in generating their concurrent systems.

This minimum amount of abstractions improves the visibility of the system

Can also increase the viscosity, any change made to 

\subsection{Threads and Shared Memory}
The threading model of concurrency typically uses 


\subsection{Apache Spark}
\subsubsection{Hidden Dependencies}

\subsubsection{Viscosity/Fluidity}

\subsubsection{Premature commitment}

\subsubsection{Role Expressiveness}

\subsubsection{Hard Mental Operations}

\section{Conclusion}

We cannot hope to offer an exhaustive analysis of the concurrent
programming landscape in this short paper, instead we have focussed on
a selection of approaches which are representative of some of the
different levels of abstraction offered to the programmer. We hope
that\ldots


% \begin{figure}
% \centering
% \epsfig{file=fly.eps, height=1in, width=1in}
% \caption{A sample black and white graphic (.eps format)
% that has been resized with the \texttt{epsfig} command.}
% \end{figure}

\bibliographystyle{abbrv}
\bibliography{distributed-computing-ozchi-2015}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

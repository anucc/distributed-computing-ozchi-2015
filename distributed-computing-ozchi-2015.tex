\documentclass{sig-alternate}
\usepackage{url}
\usepackage{tabulary}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{OzCHI}{'15, December 07-10, 2015, Melbourne, Australia}
%\CopyrightYear{2015} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Comparing usability of concurrency models}

\numberofauthors{1}
\author{
\alignauthor
Ben Swift, Peter Davis, Henry Gardner\\
\vskip .3em
\affaddr{Research School of Computer Science}\\
\affaddr{Australian National University}\\
\affaddr{Canberra, Australia}\\
\vskip .3em
\email{\{ben.swift,peter.davis,henry.gardner\}@anu.edu.au}
}

\maketitle

\begin{abstract}
Abstract goes here.
\end{abstract}

\section{ Concurrency}
This paper explores concurrency with the aim of describing concurrent programming from a HCI viewpoint. The field of concurrent processing is notable for having mutliple applications, architectures and problems that sequential programming paradigms do not have. This exploration is interesting as the primary constraint in the development of quality (as defined by \cite{9126}) concurrent systems is the inability of the programmer to develop using the prevalent concrrent tools.

We start by introducing a conceptual model similar to the Open Systems Interconnect (OSI) Model but intended for use in understanding concurency concepts. We make the argument that all implemented concurrent solutions utilise these concurrency concepts, whether or not they are explicitly or implicitly mentioned.

We then introduce 'Cognitive Dimensions', an established (by  \cite{green89:_cognit_dimen_notat}) and broad framework for evaluating cognitive systems in general, and relate some concurrent concepts in terms of the cognitive dimensions framework.

Lastly we introduce four commonly used models of concurrency: Threads, Bulk Synchronous Parallel (BSP), Communicating Sequential Processes (CSP) and Message Passing Interface (MPI), and describe these in terms of both the concurrency framework and the CD framework, making some preliminary observations about their ease of use.

\subsection{Concurrency Definitions}
There exists some confusion around definiton of concurrent systems, with some users mistaking concurrency for paralellism \cite{pike13:_concur_paral}, and others seeing asynchronous function calls as differenct concurrency

Concurrency refers to systems where there is more than one process existing at a time, whose component processes interact with each other by communication \cite{tpc}.
There exists a 

We further make the distinction between distributed and independent systems by whether the executing processes are separated by the presence of a connecting system other than a local memory or cache. Normally a network stack.

There is some debate as to the difference between asynchronous systems and parallel systems, with the former being primarily identified by the existence of non-blocking calls and the latter being identified via independently running processes.

\cite{john90:_munin} ``Shared memory programs are easier to develop than distributed memory (message passing) programs, because the programmer need not worry about the explicit movement of data.''


\section{Common Concurrency Issues}
\subsection{General Issues}
\cite{shanlu08:_learn_mistak_compr_study_real} examines the problem patterns, manifestation and solutions of 105 randomly selected real world concurrency bugs. This paper only looked at threading and pointers (as this is the dominant concurrent programming paradigm), but the issues explored are broadly applicable to all concurrent programming models.

\subsubsection{Deadlock}
Defined by \cite{shanlu08:_learn_mistak_compr_study_real} as:``Deadlock occurs when two or more operations circularly wait for each other to release the acquired resource''. Whilst being the most well known 

\subsubsection{Atomicity Violation}
Defined by \cite{shanlu08:_learn_mistak_compr_study_real} as: ``The desired serializability among multiple memory accesses is violated (i.e. a code region is intended to be atomic, but the atomicity is not enforced during execution)''. \cite{shanlu08:_learn_mistak_compr_study_real} found that out of 105 studied concurrency bugs, 

\subsubsection{Order Violation}
Defined by \cite{shanlu08:_learn_mistak_compr_study_real} as:``The desired order between two (groups of) memory accesses is flipped (i.e., A should always be executed before B, but the order is not enforced during execution)''. This is also known as a race condition. 

\subsubsection{Livelock}
Defined by \cite{oracle:_starv_livel} as when: ``Threads are not blocked - they are simply too busy responding to one another to continue work''. This issue was not identified specifically by \cite{shanlu08:_learn_mistak_compr_study_real} and so its real world prevalence is unclear.

\subsection{Distributed Issues}
The issues that distributed concurrent systems face include all of the singular system problems, but are affected by the vagaries of both a more complicated underying physical interconnection sysstem and a greater number of individual components potentially affected by failure. The eight fallacies of distributed computing are a widely used overview of the problems affecting distributed systems:

\begin{enumerate}
\item The network is reliable
\item Latency is zero
\item Bandwidth is infinite
\item The network is secure
\item Topology doesn't change
\item There is one administrator
\item The transport cost is zero
\item The network is homogenous
\end{enumerate}

\subsubsection{Consistency and Replication}
This refers to the problem of keeping global state synchronised across multiple processors, and doing so in a fashion that does not adversely affect performance.

\subsubsection{Reliability}
In non-distributed systems it is incredibly remote that calls to local memory or local cache will fail, however this is not true when running over a network. 

\section{Concurrency layers}
Introduce 'axes', with which concurrency models will be evaluated.

Similar to the OSI model of splitting the network stack by each layer, each layer relies on the previous layer in order to function properly and provide services to the next layer up.


\begin{tabulary}{\linewidth}{LLLLL}
 & Layer & Example \\ \hline
1 & Physical &  Physical computing hardware, network connnections \\
2 & Communication & MPI, ZeroMQ, Pointers \\
3 & Concurrency Model  & BSP, Actor, CSP, Threads \\
4 & Algorithmic Skeleton & Farm, pipe, map, reduce, divide and conquer\\
5 & Solution Algorithm & FFT, MapReduce, Particle-In-Cell (PIC)
\end{tabulary}

\section{Concurrency Libraries and Tools}
\subsection{MPI}
MPI was orignally conceived as an effort to standardise the increasingly disparate communication protocols that were being developed by each Massively Parallel Processor (MPP) manufacturer \cite{g.96:_pvm_mpi_compar_featur}. It quickly became much more than a communication library, providing a set of implicit concurrency models, algorithmic skeletons and performance enhancing tools. In the parlance of the concurrency layers provided above, this concurrency tool spans multiple concurrency layers (2-4) and this flexibility is likely why MPI has become the de-facto tool for supercomputing applications.

\subsection{Threads}
Threads represent independent execution contexts within a computing system. They exist at the third level of the concurrency layer, and are an operating system concurrency primitive. 

Many control structures are available to control the behaviour of threads, including locks, mutexes and semaphores.

They were the original construct used to execute concurrent programming and are arguably still the most prevalent, being the primarily method by which most operating systems coordinate the various operations that must be completed to preseent a responsive interface to the user.

\subsection{Go Channels}
Communicating Sequential Processes (CSP) is a concurrency paradigm created by \cite{Hoare:1978:CSP:359576.359585} in the late 1970's, and was designed to formalize the patterns of interaction in concurrent systems. The Go programming language was designed with concurrency in mind, and a CSP inspired message passing 'channel' system is included as a core part of the language.

\subsection{PIC}
Particle-In-Cell simulations are used by physicists when modelling and understanding particle behaviour in the presence of fields (e.g. electromagnetic). These models make use of concurrent programming in two primary ways: firstly, the simulation space can be split into separatelly simulated areas from which global results (from the whole simulation can be attained), and secondly from the method of solving the field  equations which when solved spectrally using FFT can be efficiently split between machines.

\subsection{Apache Spark}
Developed from work on Resilient Distributed Datasets (RDDs)  \cite{zaharia14:_archit_fast_gener_data_proces_large_clust},  Apache Spark is intended as a general purpose cluster computing environment. It concentrates on a particular algorithmic skeleton (master-slave or farm) and provides four concurrent tools for users of the system:

\begin{itemize}
\item RDD - A distributed and fault tolerant data storage abstraction
\item Shared Variables - Allows parallel operations to access shared state
\item Application Framework - Allows users to create parallel applications without being concerned with the lower concurrency layers
\item Interoperability - Allows data storage and retrieval from popular data storage solutions such as Hadoop or HBase
\end{itemize}

\section{HCI in concurrency}
HCI is concerned with the usage, implementation and perception of computers by humans. This paper is interested in the usability of concurrent systems. 

The increasing focus on multiprocessor and distributed systems has meant that 

Concurrent programming poses a particuarly complicated set of problems for programmers due to its non-sequential nature, being hard to reason about, hard to implement and hard to maintain. 


Usability is typically considered as a metric of software quality, and is defined by \cite{9126} as: ``The capability of the software product to be understood, learned, used and attractive to the user, when used under specified conditions.'' Normally this definition is aimed at end users encountering finished software products, rather than being aimed at the programmer who is engaged in making the software.





Programmers face a separate set of usability issues when writing software.  software patterns are notorious for being hard to reason about and hence have poor usability.

In order to explore the usability of various concurrency models we borrow the Cognitive Dimensions \cite{green89:_cognit_dimen_notat} (CD) framework, 



which was initially developed to explore the productivity of visual programming environments but has been used more broadly in areas such as programming language evaluation \cite{enpl}, and 

are the usersâ€™ intended activities adequately supported by the structure of the information artifact


We consider some of the tools used for concurrent programming and examine how these tools aim to provide a more productive, safe or understandable environment for a programmer wishing to create concurrent programs. We approach these aims from the standpoint that many of the porblems present in concurrent programming arise as a result of a lack of 'usability' of concurrent paradigms and tools.



\subsection{Hidden Dependencies}

Recovering the mental representation from the concurrent program is difficult, as there is not necessarily any sequential manner in which the program may operate. As such, for each process in a concurrent system there are hidden dependencies in the form of the other processes in the system, which may be in any number of states and producing any number of communications to the process being considered.

\subsection{Viscosity/Fluidity}
The viscoisity of a concurrent program is hard to generalize. On one hand, the program may be very fluid if it is an operation that is replicated across many nodes with little or no variance in each nodes funciton. In this case then the program is very fluid as any changes made only need to be made to a single part of the code, and this code will be replicated to all processes. However, if the concurrent program has many different nodes with different behaviours and functions then it may be a viscous procedure to any part of the program, as the flow on effects from that change will need to be tracked and accounted for in the downstream or dependent nodes.

\subsection{Premature commitment}
Concurrent programs encourage some degree of premature commitment from the programmer, particularly along the concurrecy layers. Before being able to attack the root of the probelm they are trying to solve, the programmer will need to take into account the concurrency model, the skeleton algorithm, the physical hardware available, and the communication method. Whether the problem is being solved via a directed acyclic graph (DAG) topology or via a grid topoology will determine the structure of the concurrency model, considerations such as whether messages between processes can be buffered or must be synchronous will affect how they can write the solution.

\subsection{Role Expressiveness}
Role expressiveness is related to how easily the user can discern the purpose of a particular component, and hence create a mental model of how the system works. Concurrent systems do not offer any particular advantage or disadvantage in this regard; they may provide simpler methods of expressiveness by having each process perform a single easily identifiable role, allowing the user to identify the system purpose as a simple workflow, but more complicdated concurrent systems where each process handles multiple functions within the system may cause the programmer difficulty in identify the role of particular system components. 

Increasing the role expressivenesss of a concurrent system can be thought of as decreasing the number of functions that a particular process is required to complete, thereby allowing the programmer to easily identify the role of each process and its place within the overall system. This may come a cost of less performant system, greater hardware requirements to support the greater number of simpler concurrency nodes, or may be completely impossible if the solution demands. 

\subsection{Hard Mental Operations}
Not all mental operations are equivalent, and non-sequential program execution is commonly cited as a common hard mental opeation. Many of the concurrent tools try to abstract away as much of the non-sequential operations as possible, leaving the programmer to only need to consider a set of related sequential steps. 

\section{Applying CD to Select Concurrent Models}
The CD framework allows us to examine some (hopefully) orthogonal congnitive concepts and how they relate to some concurrent models. Three models from different levels of the concurrency layer are chosen and examined.

\subsection{MPI}
MPI has a minimum amount of abstractions, using a large but fairly basic api for programmers to utilise in generating their concurrent systems.

This minimum amount of abstractions improves the visibility of the system

Can also increase the viscosity, any change made to 

\subsection{Threads & Shared Memory}
The threading model of concurrency typically uses 


\subsection{Apache Spark}
\subsubsection{Hidden Dependencies}

\subsubsection{Viscosity/Fluidity}

\subsubsection{Premature commitment}

\subsubsection{Role Expressiveness}

\subsubsection{Hard Mental Operations}

\section{Conclusion}


% \begin{figure}
% \centering
% \epsfig{file=fly.eps, height=1in, width=1in}
% \caption{A sample black and white graphic (.eps format)
% that has been resized with the \texttt{epsfig} command.}
% \end{figure}

\bibliographystyle{abbrv}
\bibliography{distributed-computing-ozchi-2015}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
